{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载库文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:39:16.196805Z",
     "start_time": "2025-04-04T06:39:14.400256Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import string\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torch.utils import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import datetime\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch import Tensor\n",
    "import torch.utils.data as Data\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.display import display\n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def set_seed(seed=2022):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(2022)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 课程与知识结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:39:16.455363Z",
     "start_time": "2025-04-04T06:39:16.201240Z"
    }
   },
   "outputs": [],
   "source": [
    "course = pd.read_pickle('data/select_data/course.pkl')[['id','name','about','resource']]\n",
    "ALL_course_id = course['id'].tolist()\n",
    "course.index = list(course['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T15:56:47.037165Z",
     "start_time": "2025-03-02T15:56:47.034356Z"
    }
   },
   "source": [
    "### 知识结构抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:39:16.496923Z",
     "start_time": "2025-04-04T06:39:16.463067Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 调整知识点\n",
    "def delete_test(one_kp):\n",
    "    if '作业' in one_kp:\n",
    "        one_kp = one_kp.split('作业')[0].strip()\n",
    "        if one_kp != '':\n",
    "            # 去除’-‘\n",
    "            for i in range(10):\n",
    "                if one_kp[-1] == '-':\n",
    "                    one_kp = one_kp[:-1].strip()\n",
    "                else:\n",
    "                    break\n",
    "            return one_kp\n",
    "        else:\n",
    "            return '作业'\n",
    "    else:\n",
    "        return one_kp.strip()\n",
    "resource = []\n",
    "for in_x, one_course in course.iterrows():\n",
    "    one_c_id = one_course['id']\n",
    "    one_c_name = one_course['name']\n",
    "    one_re_list = one_course['resource']\n",
    "    one_c_735164 = []\n",
    "    for one_re in one_re_list:\n",
    "        one_tit = one_re['titles']\n",
    "        one_re_id = one_re['resource_id'].split('_')\n",
    "        re_type = one_re_id[0]\n",
    "        kp_1 = one_tit[0]\n",
    "        kp_2 = one_tit[1]\n",
    "        kp_3 = one_tit[2]\n",
    "        if kp_2 is None:\n",
    "            if kp_3 == '作业':\n",
    "                kp_2 = kp_1\n",
    "            else:\n",
    "                kp_2 = kp_3\n",
    "        if kp_3 == 'Video' or kp_3 == 'video' or kp_3 == '作业':\n",
    "            kp_3 = kp_2\n",
    "        kp_1 = delete_test(kp_1)\n",
    "        kp_2 = delete_test(kp_2)\n",
    "        kp_3 = delete_test(kp_3)\n",
    "        if one_c_id in [746997,845950]:\n",
    "            if '--' in kp_3 and '牛顿' not in kp_3:\n",
    "                kp_3 = kp_3.split('--')[1]\n",
    "            if '--' in kp_2:\n",
    "                kp_2 = kp_2.split('--')[1]\n",
    "        if one_c_id == 1756063:\n",
    "            if 'Exercise' in kp_2:\n",
    "                kp_2 = kp_1\n",
    "            kp_3 = kp_2\n",
    "        if one_c_id == 735164:\n",
    "            if re_type == 'Ex':\n",
    "                search_str = kp_1+'#####'+kp_2\n",
    "                for one_oo in one_c_735164:\n",
    "                    if search_str in one_oo:\n",
    "                        kp_2 = one_oo.split('#####')[1]\n",
    "                        kp_3 = one_oo.split('#####')[1]\n",
    "                        break\n",
    "            else:\n",
    "                one_c_735164.append(kp_1+'#####'+kp_2)\n",
    "        if kp_3 in kp_2:\n",
    "            kp_3 = kp_2       \n",
    "        one_re_content = {'course_id':one_c_id,'re_id':int(one_re_id[1]),'re_type':re_type,\n",
    "                          'title_old':one_tit,\n",
    "                          'kp_1':kp_1,'kp_2':kp_2,'kp_3':kp_3,\n",
    "                         'chapter':one_re['chapter'],'c_name':one_c_name}\n",
    "        resource.append(one_re_content)\n",
    "resource = pd.DataFrame(resource)\n",
    "resource = pd.read_csv('data/resource_good.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:39:16.535207Z",
     "start_time": "2025-04-04T06:39:16.507777Z"
    }
   },
   "outputs": [],
   "source": [
    "ALL_EXM_id = list(resource[resource['re_type']=='Ex']['re_id'])\n",
    "ALL_VIDEO_id = list(resource[resource['re_type']=='V']['re_id'])\n",
    "print(len(ALL_EXM_id),'exms')\n",
    "print(len(ALL_VIDEO_id),'video')\n",
    "resource['kp_0'] = list(resource['c_name'])\n",
    "ALL_kp_name = []\n",
    "ALL_kp_name.extend(list(resource['kp_0']))# kp_0\n",
    "ALL_kp_name.extend(list(resource['kp_1']))\n",
    "ALL_kp_name.extend(list(resource['kp_2']))\n",
    "ALL_kp_name.extend(list(resource['kp_3']))\n",
    "ALL_kp_name = list(set(ALL_kp_name))\n",
    "print('kp_num:',len(ALL_kp_name))\n",
    "kp_name_2_id = dict(zip(ALL_kp_name,list(range(len(ALL_kp_name)))))\n",
    "kp_id_2_name = dict(zip(list(range(len(ALL_kp_name))),ALL_kp_name))\n",
    "resource['kp_id'] = resource['kp_3'].map(kp_name_2_id)\n",
    "resource['kp_0_id'] = resource['kp_0'].map(kp_name_2_id)\n",
    "resource['kp_1_id'] = resource['kp_1'].map(kp_name_2_id)\n",
    "resource['kp_2_id'] = resource['kp_2'].map(kp_name_2_id)\n",
    "resource['kp_3_id'] = resource['kp_3'].map(kp_name_2_id)\n",
    "resource.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:39:31.442301Z",
     "start_time": "2025-04-04T06:39:31.429735Z"
    }
   },
   "outputs": [],
   "source": [
    "# 抽取关系\n",
    "ALL_Ex_data = resource[resource['re_type']=='Ex'].copy()\n",
    "ALL_Video_data = resource[resource['re_type']=='V'].copy()\n",
    "# 练习与课程关系\n",
    "ex_2_course = dict(zip(ALL_Ex_data['re_id'],ALL_Ex_data['course_id']))\n",
    "# 练习与知识点关系\n",
    "ex_2_kp = dict(zip(ALL_Ex_data['re_id'],ALL_Ex_data['kp_id']))\n",
    "# 视频与课程关系\n",
    "video_2_course = dict(zip(ALL_Video_data['re_id'],ALL_Video_data['course_id']))\n",
    "# 视频与知识点关系\n",
    "video_2_kp = dict(zip(ALL_Video_data['re_id'],ALL_Video_data['kp_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T15:56:47.097860Z",
     "start_time": "2025-03-02T15:56:47.093994Z"
    }
   },
   "source": [
    "## 学生作答数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:39:37.347795Z",
     "start_time": "2025-04-04T06:39:34.287051Z"
    }
   },
   "outputs": [],
   "source": [
    "user_question = pd.read_pickle('data/select_data/user_question.pkl')\n",
    "user_question = user_question[user_question['course_id'].isin(ALL_course_id)]\n",
    "user_question['submit_time'] = pd.to_datetime(user_question['submit_time'])\n",
    "user_question['kp_id'] = user_question['ex_id'].map(ex_2_kp)\n",
    "user_question = user_question.sort_values(by=['user_id', 'submit_time'])  # 确保数据按学生和时间排序\n",
    "user_question['use_time'] = user_question.groupby('user_id')['submit_time'].diff()\n",
    "user_question['use_time'] = user_question['use_time'].dt.total_seconds()\n",
    "# 去掉时间过长的\n",
    "user_question['use_time'] = user_question['use_time']*(user_question['use_time']<600)\n",
    "user_question['use_time'] = user_question['use_time'].replace(np.NAN,0)\n",
    "mean_time_diff = user_question.groupby('user_id')['use_time'].mean()\n",
    "mean_time_diff = dict(zip(mean_time_diff.index,mean_time_diff))\n",
    "def get_mean(one_row):\n",
    "    if one_row['use_time'] == 0:\n",
    "        return mean_time_diff[one_row['user_id']]\n",
    "    else:\n",
    "        return one_row['use_time']\n",
    "user_question['use_time'] = user_question.apply(get_mean,axis=1)\n",
    "user_question.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算成绩数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:39:37.572988Z",
     "start_time": "2025-04-04T06:39:37.532138Z"
    }
   },
   "outputs": [],
   "source": [
    "# 每个人每门课程的成绩\n",
    "studentInfo = user_question.groupby(['user_id','course_id'])['is_correct'].mean().reset_index()\n",
    "studentInfo = studentInfo.drop_duplicates(['user_id'])\n",
    "stu_course_q_num = user_question.groupby(['user_id','course_id'])['problem_id'].count().reset_index()\n",
    "stu_course_q_num = stu_course_q_num.drop_duplicates(['user_id'])\n",
    "stu_2_q_num = dict(zip(list(stu_course_q_num['user_id']),list(stu_course_q_num['problem_id'])))\n",
    "studentInfo['q_num'] = studentInfo['user_id'].map(stu_2_q_num)\n",
    "is_pass = (studentInfo['is_correct']>0.95)&(studentInfo['q_num']>90)\n",
    "print('通过率：',sum(is_pass)/len(studentInfo))\n",
    "studentInfo['is_pass'] = is_pass\n",
    "studentInfo['is_pass'] = studentInfo['is_pass'].astype(int)\n",
    "ALL_STU_ID = list(studentInfo['user_id'])\n",
    "studentInfo.columns = ['stu_id','course_id','rate_pass','q_num','final_result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T17:12:56.479450Z",
     "start_time": "2025-03-02T17:12:56.475064Z"
    }
   },
   "source": [
    "## 学生视频数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:39:39.783536Z",
     "start_time": "2025-04-04T06:39:39.727439Z"
    }
   },
   "outputs": [],
   "source": [
    "user_video = pd.read_pickle('data/select_data/user_video.pkl')\n",
    "user_video = user_video[user_video['course_id'].isin(ALL_course_id)]\n",
    "user_video = user_video[user_video['user_id'].isin(ALL_STU_ID)]\n",
    "user_video = user_video.sort_values(by='ts')\n",
    "user_video['kp_id'] = user_video['video_id'].map(video_2_kp)\n",
    "user_video['use_time'] = abs(user_video['end_point'] - user_video['start_point'])\n",
    "user_video.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用户评论数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:39:44.407725Z",
     "start_time": "2025-04-04T06:39:44.371796Z"
    }
   },
   "outputs": [],
   "source": [
    "user_comment = pd.read_pickle('data/select_data/user_comment.pkl')\n",
    "user_comment = user_comment[user_comment['user_id'].isin(ALL_STU_ID)]\n",
    "user_comment['create_time'] = pd.to_datetime(user_comment['create_time'])\n",
    "user_comment = user_comment.sort_values(by='create_time')\n",
    "user_comment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习行为聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:39:57.089306Z",
     "start_time": "2025-04-04T06:39:46.255431Z"
    }
   },
   "outputs": [],
   "source": [
    "studentInfo['stu_id_index'] = list(studentInfo['stu_id'])\n",
    "\n",
    "ALL_STU_Start = {}\n",
    "# ['date', 'sum_click', 'activity_type', 'stu_id', 'stu_id_index']\n",
    "studentVle = []\n",
    "comm_col = ['stu_id','id_site','kp_id','course_id','ts']\n",
    "for one_stu_id in tqdm(studentInfo['stu_id']):\n",
    "    one_q_data = user_question[user_question['user_id']==one_stu_id].copy()\n",
    "    one_q_data = one_q_data[['user_id','ex_id','kp_id','course_id','submit_time']]\n",
    "    one_q_data.columns = comm_col\n",
    "    one_q_data['activity_type'] = 'question'\n",
    "    one_q_data['sum_click'] = 1\n",
    "    \n",
    "    one_video_data = user_video[user_video['user_id']==one_stu_id].copy()\n",
    "    one_video_data = one_video_data[['user_id','video_id','kp_id','course_id','ts']]\n",
    "    one_video_data.columns = comm_col\n",
    "    one_video_data['activity_type'] = 'video'\n",
    "    one_video_data['sum_click'] = 1\n",
    "    \n",
    "    one_comment_data = user_comment[user_comment['user_id']==one_stu_id].copy()\n",
    "    one_comment_data = one_comment_data[['user_id','id','create_time']]\n",
    "    one_comment_data.columns = ['stu_id','id_site','ts']\n",
    "    one_comment_data['activity_type'] = 'comment'\n",
    "    one_comment_data['sum_click'] = 1\n",
    "    one_stu_data = pd.concat([one_q_data, one_video_data, one_comment_data], axis=0)\n",
    "    one_stu_data = one_stu_data.sort_values(by='ts').reset_index(drop=True)\n",
    "    \n",
    "    ALL_STU_Start[one_stu_id] = one_stu_data['ts'][0]\n",
    "    one_stu_data['date'] = one_stu_data['ts'] - one_stu_data['ts'][0]\n",
    "    one_stu_data['date'] = one_stu_data['date'].dt.days + 1 \n",
    "    one_stu_data['stu_id_index'] = list(one_stu_data['stu_id'])\n",
    "    studentVle.append(one_stu_data)\n",
    "studentVle = pd.concat(studentVle, axis=0)\n",
    "studentVle = studentVle.sort_values(by='ts').reset_index(drop=True)\n",
    "studentVle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:39:58.559157Z",
     "start_time": "2025-04-04T06:39:58.537479Z"
    }
   },
   "outputs": [],
   "source": [
    "# 每个学生，每个视频看的总时长\n",
    "user_video_time_sum = user_video.groupby(['user_id','video_id'])['use_time'].sum().reset_index()\n",
    "# 每个学生看每个视频的平均时长\n",
    "user_video_time_mean = user_video_time_sum.groupby('user_id')['use_time'].mean()\n",
    "# 每个视频的平均观看时长\n",
    "video_time_mean = user_video_time_sum.groupby('video_id')['use_time'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:40:00.986832Z",
     "start_time": "2025-04-04T06:40:00.975672Z"
    }
   },
   "outputs": [],
   "source": [
    "# 更新资源信息\n",
    "resource['video_t_mean'] = resource['re_id'].map(dict(video_time_mean))\n",
    "studentInfo['video_t_mean'] = studentInfo['stu_id'].map(dict(user_video_time_mean))\n",
    "studentInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 危险学生预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:42:07.165068Z",
     "start_time": "2025-03-11T13:42:07.137790Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_count_data(one_set):\n",
    "    # 获取次数\n",
    "    all_stu_data = []\n",
    "    one_set.index = list(one_set['stu_id_index'])\n",
    "    one_set_stu = one_set['stu_id_index'].value_counts()\n",
    "    one_stu_data_0 = dict(zip(ALL_ACT_TYPE,[0]*len(ALL_ACT_TYPE)))\n",
    "    for one_stu in ALL_STU_ID:\n",
    "        one_stu_data = one_stu_data_0.copy()\n",
    "        if one_stu in one_set_stu.index:\n",
    "            one_stu_set = one_set.loc[one_stu]\n",
    "            if one_set_stu[one_stu] ==1:\n",
    "                one_stu_data[one_stu_set['activity_type']] = one_stu_set['sum_click']\n",
    "            else:\n",
    "                one_stu_data.update(dict(one_stu_set.groupby('activity_type').sum()['sum_click']))\n",
    "        all_stu_data.append(one_stu_data)\n",
    "    one_re = pd.DataFrame(all_stu_data,index=ALL_STU_ID)\n",
    "    return one_re\n",
    "\n",
    "def get_tf_idf(count_data):\n",
    "    # 获取相应的tf-idf值\n",
    "    all_stu_len = sum(count_data.sum(axis=1)!=0)\n",
    "    all_idf = {}\n",
    "    for one_act_type in ALL_ACT_TYPE:\n",
    "        one_num = sum(count_data[one_act_type]!=0)\n",
    "        all_idf[one_act_type] = math.log(all_stu_len/(one_num+1),10)\n",
    "    all_stu_data = []\n",
    "    one_tf_0 = dict(zip(ALL_ACT_TYPE,[0]*len(ALL_ACT_TYPE)))\n",
    "    for one_stu in ALL_STU_ID:\n",
    "        one_stu_data = count_data.loc[one_stu]\n",
    "        one_sum = sum(one_stu_data)\n",
    "        if one_sum!=0:\n",
    "            one_tf = dict(one_stu_data/one_sum) # 字典\n",
    "        else:\n",
    "            one_tf = one_tf_0.copy() # 字典\n",
    "        one_tf_idf = [one_tf[one_a]*all_idf[one_a] for one_a in ALL_ACT_TYPE]\n",
    "        one_tf_idf = dict(zip(ALL_ACT_TYPE,one_tf_idf))\n",
    "        all_stu_data.append(one_tf_idf)\n",
    "    one_re = pd.DataFrame(all_stu_data,index=ALL_STU_ID)\n",
    "    return one_re\n",
    "\n",
    "def save_data(stpe_day,is_single,is_count,step_num,one_data_set):\n",
    "    data_path = f'data/risk_stu/{STEP_DAY}/{is_single}/{is_count}'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    flie_path = f'data/risk_stu/{STEP_DAY}/{is_single}/{is_count}/{one_step}.pkl'\n",
    "    data_set = one_data_set.copy()\n",
    "    data_set['stu_id_index'] = ALL_STU_ID\n",
    "    data_set.to_pickle(flie_path)\n",
    "    \n",
    "def performance_risk(ground_truth, prediction):\n",
    "    auc = metrics.roc_auc_score(ground_truth, prediction) * 100\n",
    "    # 转成0，1\n",
    "    prediction = np.round(prediction)\n",
    "    f1 = metrics.f1_score(ground_truth, prediction) * 100\n",
    "    recall = metrics.recall_score(ground_truth, prediction) * 100\n",
    "    precision = metrics.precision_score(ground_truth, prediction) * 100\n",
    "    acc = metrics.accuracy_score(ground_truth, prediction) * 100\n",
    "    rmse = np.sqrt(mean_squared_error(ground_truth,prediction))\n",
    "    one_re_t = [auc, f1, recall, precision, acc,rmse]\n",
    "    re_col = ['Auc', 'F1', 'Recall', 'Precision', 'Acc','Rmse']\n",
    "    one_re_dict = dict(zip(re_col,one_re_t))\n",
    "    return one_re_dict\n",
    "\n",
    "class LSTM_DataSet(Dataset):\n",
    "    def __init__(self,all_stu_data,week_need):\n",
    "        self.all_stu_data = all_stu_data\n",
    "        self.week_need = week_need\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_stu_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        one_data = self.all_stu_data[index]\n",
    "        x = torch.FloatTensor(one_data['week_data'][:self.week_need])\n",
    "        comm_data = torch.FloatTensor(one_data['common_data'])\n",
    "        y = torch.as_tensor(one_data['final_result'], dtype=torch.float)\n",
    "        return x,y,comm_data\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim,is_info,info_len,model_name):\n",
    "        super(LSTMModel, self).__init__()        \n",
    "        # 定义 LSTM 层\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.is_info = is_info\n",
    "        self.info_len = info_len\n",
    "        self.model_name = model_name\n",
    "        predict_dim = hidden_dim\n",
    "        if model_name == 'LSTM':\n",
    "            self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True) \n",
    "        elif model_name == 'BiLSTM':\n",
    "            predict_dim += hidden_dim\n",
    "            self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True, bidirectional=True) \n",
    "        elif model_name == 'RNN':\n",
    "            self.lstm = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        # 定义全连接层\n",
    "        if is_info:\n",
    "            predict_dim += hidden_dim\n",
    "            self.info_layer = nn.Sequential(nn.Linear(self.info_len, hidden_dim),\n",
    "                                             nn.ReLU(True),\n",
    "                                             nn.Linear(hidden_dim, hidden_dim))     \n",
    "        self.class_classifier = nn.Sequential(nn.Linear(predict_dim, predict_dim),\n",
    "                                             nn.ReLU(True),\n",
    "                                             nn.Linear(predict_dim, 1),\n",
    "                                             nn.Sigmoid())\n",
    "    def forward(self, x,common_data):\n",
    "        if self.model_name == 'LSTM':\n",
    "            hidden_len = self.layer_dim\n",
    "            h0 = torch.zeros(hidden_len, x.size(0), self.hidden_dim).to(DEVICE)\n",
    "            c0 = torch.zeros(hidden_len, x.size(0), self.hidden_dim).to(DEVICE)\n",
    "            out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        elif self.model_name == 'BiLSTM':\n",
    "            hidden_len = self.layer_dim *2\n",
    "            h0 = torch.zeros(hidden_len, x.size(0), self.hidden_dim).to(DEVICE)\n",
    "            c0 = torch.zeros(hidden_len, x.size(0), self.hidden_dim).to(DEVICE)\n",
    "            out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        elif self.model_name == 'RNN':\n",
    "            hidden_len = self.layer_dim\n",
    "            h0 = torch.zeros(hidden_len, x.size(0), self.hidden_dim).to(DEVICE)\n",
    "            out, _ = self.lstm(x, h0)\n",
    "        out = out[:, -1, :]\n",
    "        if self.is_info:\n",
    "            out = torch.cat([out, self.info_layer(common_data)], dim=1)\n",
    "        out = self.class_classifier(out)\n",
    "        out = out.squeeze()\n",
    "        return out\n",
    "\n",
    "def get_stand_data(one_data_set,use_info):\n",
    "    scaler = StandardScaler()\n",
    "    need_data = one_data_set[use_info]\n",
    "    df_standardized = pd.DataFrame(scaler.fit_transform(need_data), columns=use_info)\n",
    "    df_standardized['stu_id_index'] = list(one_data_set['stu_id_index'])\n",
    "    df_standardized.index = list(df_standardized['stu_id_index'])\n",
    "    return df_standardized\n",
    "\n",
    "def get_time_data(need_stu_id,data_path,step_num,stu_info_set,stu_info,act_type,is_front,is_stand,is_merge,count_type):\n",
    "    all_week_data = []\n",
    "    for one_week in range(1,step_num+1):\n",
    "        one_data = pd.read_pickle(f'{data_path}/{one_week}.pkl')\n",
    "        if is_stand:\n",
    "            one_data = get_stand_data(one_data,act_type)\n",
    "        all_week_data.append(one_data)\n",
    "    if is_merge:\n",
    "        a_conut = 'count' if count_type == 'tf' else 'tf'\n",
    "        new_path = data_path.replace(count_type,a_conut)\n",
    "        all_week_data_a = []\n",
    "        for one_week in range(1,step_num+1):\n",
    "            one_data = pd.read_pickle(f'{new_path}/{one_week}.pkl')\n",
    "            if is_stand:\n",
    "                one_data = get_stand_data(one_data,act_type)\n",
    "            all_week_data_a.append(one_data)\n",
    "    all_stu_data = []\n",
    "    for one_stu in tqdm(need_stu_id,desc='read_data'):\n",
    "        one_stu_data = {'final_result':stu_info_set.loc[one_stu]['final_result'],'week_data':[],'stu_id':one_stu}\n",
    "        one_stu_data['common_data'] = list(stu_info_set.loc[one_stu][stu_info])\n",
    "        for one_week in range(step_num):\n",
    "            one_week_data = list(all_week_data[one_week].loc[one_stu][act_type])\n",
    "            if is_merge:\n",
    "                one_week_data.extend(list(all_week_data_a[one_week].loc[one_stu][act_type]))\n",
    "            if is_front:\n",
    "                one_week_data.extend(one_stu_data['common_data'].copy())\n",
    "            one_stu_data['week_data'].append(one_week_data)\n",
    "        all_stu_data.append(one_stu_data)\n",
    "    return all_stu_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:42:27.848282Z",
     "start_time": "2025-03-11T13:42:07.166795Z"
    }
   },
   "outputs": [],
   "source": [
    "ALL_ACT_TYPE = ['video','comment','question']\n",
    "STEP_DAY = 7\n",
    "STEP_NUM = max(studentVle['date'])//STEP_DAY\n",
    "print('最大周数',STEP_NUM)\n",
    "day_path = f'data/risk_stu/{STEP_DAY}'\n",
    "print(\"开始生成数据\")\n",
    "one_step_conut_add = []\n",
    "for one_step in tqdm(range(1,STEP_NUM+1)):\n",
    "    # 获取每个周期的计数数据\n",
    "    one_step_set = studentVle[(studentVle['date']>one_step*(STEP_DAY-1)) & (studentVle['date']<=one_step*STEP_DAY)].copy()\n",
    "    one_step_count_single = get_count_data(one_step_set)\n",
    "    # 获取累积周期的计数数据\n",
    "    if one_step == 1:\n",
    "        one_step_conut_add = one_step_count_single.copy()\n",
    "    else:\n",
    "        one_step_conut_add = one_step_conut_add+one_step_count_single\n",
    "    # 获取每个周期的TF数据\n",
    "    one_step_tf_single = get_tf_idf(one_step_count_single)\n",
    "    # 获取累积周期的TF数据\n",
    "    one_step_tf_add = get_tf_idf(one_step_conut_add)\n",
    "    save_data(STEP_DAY,'single','count',one_step,one_step_count_single)\n",
    "    save_data(STEP_DAY,'add','count',one_step,one_step_conut_add)\n",
    "    save_data(STEP_DAY,'single','tf',one_step,one_step_tf_single)\n",
    "    save_data(STEP_DAY,'add','tf',one_step,one_step_tf_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:51:22.023000Z",
     "start_time": "2025-04-04T08:51:22.007110Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "studentInfo.index = list(studentInfo['stu_id_index'])\n",
    "studentInfo.head()\n",
    "STU_INFO = ['stu_id_index']\n",
    "# 参数设置\n",
    "EPOCH_NUM = 50\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cpu'\n",
    "STEP_DAY = 7 # 每隔几天采集一次数据\n",
    "STEP_NUM = 15# 运行的周数\n",
    "IS_COUNT = 1 # 是否使用conut值\n",
    "IS_ADD = 1 # 是否采用累加数据\n",
    "USE_INFO = 0 # 是否使用学生信息\n",
    "IS_MERGE = 1 # 是否把tf和count合并到一起\n",
    "IS_FRONT = 0 # 是否在LSTM里面加入学生信息\n",
    "IS_STAND = 1 # 是否归一化\n",
    "COUNT_TYPE = 'count' if IS_COUNT else 'tf'\n",
    "ADD_TYPE = 'add'if IS_ADD else 'single'\n",
    "ALL_ML_MODEL = ['RFC','KNN','DT','GNB','SVM','GBT','LR']\n",
    "DEEP_MODEL = ['LSTM','RNN']\n",
    "MODEL_NAME = 'RFC'\n",
    "config = {\n",
    "    'Model': MODEL_NAME,\n",
    "    'Add_type': ADD_TYPE,\n",
    "    'Count_type': COUNT_TYPE,\n",
    "    'USE_INFO': USE_INFO,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'IS_FRONT': IS_FRONT,\n",
    "    'IS_STAND': IS_STAND,\n",
    "    'IS_MERGE': IS_MERGE\n",
    "}\n",
    "logging.shutdown()\n",
    "time_now = datetime.datetime.now()\n",
    "now_str = time_now.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "log_dir = f'log/risk_stu/{STEP_DAY}/{MODEL_NAME}/{COUNT_TYPE}'\n",
    "log_file_name = f'{log_dir}/{ADD_TYPE}-{now_str}.log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "file_logger = logging.getLogger(log_file_name)\n",
    "file_handler = logging.FileHandler(log_file_name, mode='a', encoding='utf-8')\n",
    "file_handler.setFormatter(formatter)\n",
    "file_logger.addHandler(file_handler)\n",
    "file_logger.info(f\"{config}\")\n",
    "TRAIN_DATA, TEST_DATA = train_test_split(studentInfo,test_size=0.2, random_state=42, stratify=studentInfo['final_result'])\n",
    "TRAIN_DATA_STU = list(TRAIN_DATA['stu_id'])\n",
    "TEST_DATA_STU = list(TEST_DATA['stu_id'])\n",
    "\n",
    "TRAIN_DATA = list(TRAIN_DATA['stu_id_index'])\n",
    "print('TRAIN_DATA len',len(TRAIN_DATA))\n",
    "TEST_DATA = list(TEST_DATA['stu_id_index'])\n",
    "print('TEST_DATA len',len(TEST_DATA))\n",
    "TEST_DATA_raw = studentInfo[studentInfo['stu_id_index'].isin(TEST_DATA)].copy()\n",
    "ALL_STEP_RE = []\n",
    "for ONE_STEP in range(1,STEP_NUM+1): \n",
    "    DATA_PATH = f'data/risk_stu/{STEP_DAY}/{ADD_TYPE}/{COUNT_TYPE}'\n",
    "    best_one_re_dict = {}\n",
    "    if MODEL_NAME in ALL_ML_MODEL:\n",
    "        one_data = pd.read_pickle(f'{DATA_PATH}/{ONE_STEP}.pkl')\n",
    "        ML_data = pd.concat([studentInfo,one_data],axis=1)\n",
    "        ML_col = ALL_ACT_TYPE.copy()\n",
    "        ML_col.extend(STU_INFO)\n",
    "        X_train,y_train = ML_data.loc[TRAIN_DATA][ML_col].values,ML_data.loc[TRAIN_DATA]['final_result']\n",
    "        X_test,y_test = ML_data.loc[TEST_DATA][ML_col].values,ML_data.loc[TEST_DATA]['final_result']\n",
    "        if MODEL_NAME=='RFC':\n",
    "            model = RandomForestClassifier(n_estimators=40,max_depth=18,random_state=10)\n",
    "        if MODEL_NAME=='KNN':\n",
    "            model = KNeighborsClassifier()\n",
    "        if MODEL_NAME=='DT':\n",
    "            model = DecisionTreeClassifier(max_depth=7,random_state=2023)\n",
    "        if MODEL_NAME=='GNB':\n",
    "            model =  GaussianNB()\n",
    "        if MODEL_NAME=='SVM':\n",
    "            model = SVC(kernel='rbf',probability=True)\n",
    "        if MODEL_NAME=='GBT':\n",
    "            model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "        if MODEL_NAME=='LR':\n",
    "            model = LogisticRegression(max_iter=50)\n",
    "        model = model.fit(X_train,y_train)\n",
    "        y_train_predict = model.predict_proba(X_train)[:,1]\n",
    "        one_re_dict = performance_risk(y_train,y_train_predict)\n",
    "        file_logger.info(f\"STEP {ONE_STEP} Train: {one_re_dict}\")\n",
    "        y_test_predict = model.predict_proba(X_test)[:,1]\n",
    "        best_one_re_dict = performance_risk(y_test,y_test_predict)\n",
    "        TEST_DATA_raw[f'week_{ONE_STEP}'] = list(y_test_predict)\n",
    "        file_logger.info(f\"STEP {ONE_STEP} Test: {best_one_re_dict}\")\n",
    "    if MODEL_NAME in ['LSTM','RNN']:\n",
    "        file_logger.info(f'Start Week :{ONE_STEP}')\n",
    "        data_params = {\n",
    "            'data_path': DATA_PATH,\n",
    "            'step_num': ONE_STEP,\n",
    "            'stu_info_set': studentInfo,\n",
    "            'stu_info': STU_INFO,\n",
    "            'act_type': ALL_ACT_TYPE,\n",
    "            'is_front': IS_FRONT,\n",
    "            'is_stand': IS_STAND,\n",
    "            'is_merge': IS_MERGE,\n",
    "            'count_type': COUNT_TYPE\n",
    "        }\n",
    "        train_data = get_time_data(TRAIN_DATA,**data_params)\n",
    "        train_data = LSTM_DataSet(train_data,ONE_STEP)\n",
    "        test_data = get_time_data(TEST_DATA,**data_params)\n",
    "        test_data = LSTM_DataSet(test_data,ONE_STEP)\n",
    "        train_laoder = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_laoder = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        input_dim = len(ALL_ACT_TYPE)\n",
    "        if IS_FRONT:\n",
    "            input_dim += len(STU_INFO)# 输入维度\n",
    "        if IS_MERGE:\n",
    "            input_dim += len(ALL_ACT_TYPE)\n",
    "        hidden_dim = 8  # 隐藏状态维度\n",
    "        layer_dim = 1  # LSTM层数\n",
    "        output_dim = 1  # 输出维度\n",
    "        info_len = len(STU_INFO)\n",
    "        LR = 0.00001\n",
    "        model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim,USE_INFO,info_len,MODEL_NAME)\n",
    "#         criterion = nn.MSELoss()\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-8)\n",
    "        file_logger.info(f'input_dim:{input_dim}, hidden_dim:{hidden_dim}, layer_dim:{hidden_dim}, output_dim:{output_dim},info_len:{info_len}')\n",
    "        model.to(DEVICE)\n",
    "        best_auc = 0\n",
    "        best_ecoch = 0\n",
    "        # 训练模型\n",
    "        for one_epoch in range(EPOCH_NUM):\n",
    "            model.train()\n",
    "            ground_truth = torch.Tensor([])\n",
    "            prediction = torch.Tensor([])\n",
    "            for inputs, labels,common_data in tqdm(train_laoder, desc=f'Week {ONE_STEP} Train-{one_epoch}:',):\n",
    "                inputs.to(DEVICE)\n",
    "                labels.to(DEVICE)\n",
    "                common_data.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs,common_data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                prediction = torch.cat([prediction, outputs.to('cpu')])\n",
    "                ground_truth = torch.cat([ground_truth, labels.to('cpu')])\n",
    "            one_re_dict = performance_risk(ground_truth.detach().numpy(), prediction.detach().numpy())\n",
    "            file_logger.info(f'Week {ONE_STEP} Train-{one_epoch} Preformance:{one_re_dict}')\n",
    "            model.eval()\n",
    "            ground_truth = torch.Tensor([])\n",
    "            prediction = torch.Tensor([])\n",
    "            for inputs, labels,common_data in tqdm(test_laoder, desc=f'Week {ONE_STEP} Test-{one_epoch}::',):\n",
    "                inputs.to(DEVICE)\n",
    "                common_data.to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs,common_data)\n",
    "                    prediction = torch.cat([prediction, outputs.to('cpu')])\n",
    "                    ground_truth = torch.cat([ground_truth, labels.to('cpu')])\n",
    "            one_re_dict = performance_risk(ground_truth.detach().numpy(), prediction.detach().numpy())\n",
    "            file_logger.info(f'Week {ONE_STEP}  Test-{one_epoch}Preformance:{one_re_dict}')\n",
    "            if one_re_dict['Auc'] > best_auc:\n",
    "                best_auc = one_re_dict['Auc']\n",
    "                best_ecoch = one_epoch\n",
    "                file_logger.info(f'Best_acu in Valid:【{best_auc}】 at epoch 【{one_epoch}】')\n",
    "                best_one_re_dict = one_re_dict.copy()\n",
    "                TEST_DATA_raw[f'week_{ONE_STEP}'] = list(prediction.detach().numpy())\n",
    "            if one_epoch-best_ecoch>10:\n",
    "                print('Train stop')\n",
    "                break\n",
    "    best_one_re_dict['STEP'] = ONE_STEP\n",
    "    file_logger.info(f'Test【{ONE_STEP}】:{best_one_re_dict}')\n",
    "    ALL_STEP_RE.append(best_one_re_dict)\n",
    "file_logger.info(f\"{config}\")\n",
    "ALL_STEP_RE_set = pd.DataFrame(ALL_STEP_RE)\n",
    "print(ALL_STEP_RE_set)\n",
    "ALL_STEP_RE_set.to_csv(f'{log_dir}/{MODEL_NAME}-{ADD_TYPE}-{now_str}.csv')\n",
    "logging.shutdown()\n",
    "ALL_STEP_RE_set[['Auc', 'F1', 'Recall', 'Precision', 'Acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 知识追踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:42:30.558040Z",
     "start_time": "2025-03-11T13:42:30.542342Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据，划分数据\n",
    "class MyDataset_kt(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.all_item = X[USE_ITEM[:-1]].values  \n",
    "        self.all_label = X['is_right'].values  \n",
    "        self.all_index = list(X['index'])\n",
    "        del X\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = torch.as_tensor(self.all_item[index], dtype=torch.float)  \n",
    "        label = torch.as_tensor(self.all_label[index], dtype=torch.float)\n",
    "        one_index = int(self.all_index[index]) \n",
    "        return item, label, one_index\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据集大小\n",
    "        return len(self.all_item)\n",
    "\n",
    "\n",
    "def standard_scaler_kt(DF):\n",
    "    st_sc = StandardScaler()\n",
    "    scaler_data = DF[USE_ITEM[:-1]]\n",
    "    add_data = DF[['is_right','index']]\n",
    "    scaler_data_x = st_sc.fit_transform(scaler_data)\n",
    "    scaler_data_ed = pd.DataFrame(scaler_data_x, columns=USE_ITEM[:-1])\n",
    "    scaler_data_ed = scaler_data_ed.join(add_data)\n",
    "    return scaler_data_ed\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.class_classifier = nn.Sequential(nn.Linear(input_dim, input_dim*2),\n",
    "                                             nn.ReLU(True),\n",
    "                                             nn.Linear(input_dim*2, input_dim*4),\n",
    "                                             nn.ReLU(True),\n",
    "                                             nn.Linear(input_dim*4, input_dim*2),\n",
    "                                             nn.ReLU(True),\n",
    "                                             nn.Linear(input_dim*2, 10),\n",
    "                                             nn.ReLU(True),\n",
    "                                             nn.Linear(10, 1),\n",
    "                                             nn.Sigmoid())\n",
    "    def forward(self, item):\n",
    "        out = self.class_classifier(item)\n",
    "        out = out.squeeze()\n",
    "        return out\n",
    "    \n",
    "\n",
    "# 计算性能\n",
    "def performance_kt(ground_truth, prediction):\n",
    "    ground_truth = ground_truth.to('cpu')\n",
    "    prediction = prediction.to('cpu')\n",
    "    ground_truth = ground_truth.detach().numpy()\n",
    "    prediction = prediction.detach().numpy()\n",
    "    auc = metrics.roc_auc_score(ground_truth, prediction) * 100\n",
    "    # 转成0，1\n",
    "    prediction = np.round(prediction)\n",
    "    f1 = metrics.f1_score(ground_truth, prediction) * 100\n",
    "    recall = metrics.recall_score(ground_truth, prediction) * 100\n",
    "    precision = metrics.precision_score(ground_truth, prediction) * 100\n",
    "    acc = metrics.accuracy_score(ground_truth, prediction) * 100\n",
    "    rmse = np.sqrt(mean_squared_error(ground_truth,prediction))\n",
    "    return auc, f1, recall, precision, acc,rmse\n",
    "\n",
    "def __load_model__(ckpt):\n",
    "    if os.path.isfile(ckpt):\n",
    "        checkpoint = torch.load(ckpt)\n",
    "        print(\"Successfully loaded checkpoint '%s'\" % ckpt)\n",
    "        return checkpoint\n",
    "    else:\n",
    "        raise Exception(\"No checkpoint found at '%s'\" % ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据整合与侧面信息提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:52:13.508732Z",
     "start_time": "2025-04-04T08:52:13.497094Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tqdm_ncols = 80\n",
    "def updata_data(old_data, one_key, one_row_data):\n",
    "    if one_key not in old_data.keys():\n",
    "        old_data[one_key] = [\n",
    "            one_row_data['is_right'], one_row_data['use_time'], 1\n",
    "        ]\n",
    "        return old_data, [0, 0, 0]\n",
    "    else:\n",
    "        one_data = old_data[one_key]\n",
    "        re_data = [(one_data[0] / one_data[2]) * 100,\n",
    "                   one_data[1] / one_data[2], one_data[2]]\n",
    "        one_data[0] += one_row_data['is_right']\n",
    "        one_data[1] += one_row_data['use_time']\n",
    "        one_data[2] += 1\n",
    "        old_data[one_key] = one_data\n",
    "        return old_data, re_data\n",
    "\n",
    "\n",
    "def updata_answer_data(old_data, one_key, one_row_data):\n",
    "    if one_key not in old_data.keys():\n",
    "        old_data[one_key] = [\n",
    "            one_row_data['is_right'], one_row_data['use_time'], 1\n",
    "        ]\n",
    "        return old_data, [\n",
    "            one_row_data['is_right'] * 100, one_row_data['use_time'], 1\n",
    "        ]\n",
    "    else:\n",
    "        one_data = old_data[one_key]\n",
    "        one_data[0] += one_row_data['is_right']\n",
    "        one_data[1] += one_row_data['use_time']\n",
    "        one_data[2] += 1\n",
    "        old_data[one_key] = one_data\n",
    "        re_data = [(one_data[0] / one_data[2]) * 100,\n",
    "                   one_data[1] / one_data[2], one_data[2]]\n",
    "        return old_data, re_data\n",
    "\n",
    "\n",
    "def get_all_info(all_info, col_name, col_value, new_record_info):\n",
    "    if col_value not in all_info.keys():\n",
    "        one_q_data = new_record_info[new_record_info[col_name] == col_value]\n",
    "        all_info[col_value] = [\n",
    "            np.mean(one_q_data['is_right']),\n",
    "            np.mean(one_q_data['use_time']),\n",
    "            len(one_q_data.index)\n",
    "        ]\n",
    "    return all_info\n",
    "\n",
    "\n",
    "def updata_time_data(old_data, one_key, create_time_now):\n",
    "    if one_key not in old_data.keys():\n",
    "        old_data[one_key] = create_time_now\n",
    "        return old_data, 0\n",
    "    else:\n",
    "        old_time = old_data[one_key]\n",
    "        re_data = create_time_now - old_time\n",
    "        old_data[one_key] = create_time_now\n",
    "        return old_data, re_data\n",
    "    \n",
    "def updata_passed_data(old_data, one_key, now_index):\n",
    "    if one_key not in old_data.keys():\n",
    "        old_data[one_key] = [now_index]\n",
    "        return old_data, []\n",
    "    else:\n",
    "        re_data = old_data[one_key].copy()\n",
    "        old_data[one_key].append(now_index)\n",
    "        return old_data, re_data\n",
    "\n",
    "\n",
    "def get_data(raw_data, stu_kp_data, stu_q_data, kp_data, q_data, q_all_data,\n",
    "             kp_all_data, stu_kp_data_an, stu_q_data_an, kp_data_an,\n",
    "             q_data_an,last_kp_time,last_q_time,passed_kp_list,passed_q_list):\n",
    "    all_data = []\n",
    "    for i in tqdm(raw_data.index,\n",
    "                       desc='Process',\n",
    "                       mininterval=0.2,\n",
    "                       ncols=tqdm_ncols):\n",
    "        one_data = []\n",
    "        one_re_data = raw_data.loc[i]\n",
    "        stu_id = one_re_data['stu_id']\n",
    "        kp_id = one_re_data['kp_id']\n",
    "        q_id = one_re_data['q_id']\n",
    "        stu_kp = str(int(stu_id)) + '_' + str(int(kp_id))\n",
    "        one_time = one_re_data['create_time']\n",
    "\n",
    "        stu_kp_data, one_stu_kp_data = updata_data(\n",
    "            stu_kp_data,\n",
    "            stu_kp, one_re_data)\n",
    "        one_data.extend(one_stu_kp_data)\n",
    "\n",
    "        # 'stu_q_now_acc', 'stu_q_now_time', 'stu_q_now_len'\n",
    "        stu_q_data, one_stu_q_data = updata_data(stu_q_data, str(int(stu_id)),\n",
    "                                                 one_re_data)\n",
    "        one_data.extend(one_stu_q_data)\n",
    "\n",
    "        # 'q_now_acc','q_now_use_time','q_now_len'\n",
    "        q_data, one_q_data = updata_data(q_data, str(int(q_id)), one_re_data)\n",
    "        one_data.extend(one_q_data)\n",
    "        \n",
    "        # 'kp_now_acc','kp_now_use_time','kp_now_len'\n",
    "        kp_data, one_kp_data = updata_data(kp_data, str(int(kp_id)),\n",
    "                                           one_re_data)\n",
    "        one_data.extend(one_kp_data)\n",
    "\n",
    "        # 'q_all_acc','q_all_use_time','q_all_len'\n",
    "        q_all_data = get_all_info(q_all_data, 'q_id', q_id, raw_data)\n",
    "        one_data.extend(q_all_data[q_id])\n",
    "\n",
    "        # 'kp_all_acc','kp_all_use_time','kp_all_len'\n",
    "        kp_all_data = get_all_info(kp_all_data, 'kp_id', kp_id, raw_data)\n",
    "        one_data.extend(kp_all_data[kp_id])\n",
    "\n",
    "        # 'stu_kp_now_acc_an','stu_kp_now_use_time_an','stu_kp_now_len_an'\n",
    "        stu_kp_data_an, one_stu_kp_data_an = updata_answer_data(\n",
    "            stu_kp_data_an,\n",
    "            str(int(stu_id)) + '_' + str(int(kp_id)), one_re_data)\n",
    "        one_data.extend(one_stu_kp_data_an)\n",
    "\n",
    "        # 'stu_q_now_acc_an', 'stu_q_now_time_an', 'stu_q_now_len_an'\n",
    "        stu_q_data_an, one_stu_q_data_an = updata_answer_data(\n",
    "            stu_q_data_an, str(int(stu_id)), one_re_data)\n",
    "        one_data.extend(one_stu_q_data_an)\n",
    "\n",
    "        # 'q_now_acc_an','q_now_use_time_an','q_now_len_an'\n",
    "        q_data_an, one_q_data_an = updata_answer_data(q_data_an,\n",
    "                                                      str(int(q_id)),\n",
    "                                                      one_re_data)\n",
    "        one_data.extend(one_q_data_an)\n",
    "        # 'kp_now_acc_an','kp_now_use_time_an','kp_now_len_an'\n",
    "        kp_data_an, one_kp_data_an = updata_answer_data(\n",
    "            kp_data_an, str(int(kp_id)), one_re_data)\n",
    "        one_data.extend(one_kp_data_an)\n",
    "        \n",
    "        # 'last_kp_time_passed','last_q_time_passed'\n",
    "        last_kp_time,last_kp_time_passed = updata_time_data(last_kp_time,stu_kp, one_time)\n",
    "        last_q_time,last_q_time_passed = updata_time_data(last_q_time,str(int(stu_id)),one_time)\n",
    "        one_data.extend([last_kp_time_passed,last_q_time_passed])\n",
    "        \n",
    "        # 'passed_kp','passed_q'\n",
    "        passed_kp_list,passed_kp = updata_passed_data(passed_kp_list,stu_kp,i)\n",
    "        passed_q_list,passed_q = updata_passed_data(passed_q_list,str(int(stu_id)),i)\n",
    "        one_data.extend([passed_kp,passed_q])\n",
    "        \n",
    "        all_data.append(one_data)\n",
    "    all_data_2 = np.array(all_data,dtype=object)\n",
    "    new_col = [\n",
    "        'stu_kp_now_acc', 'stu_kp_now_use_time', 'stu_kp_now_len',\n",
    "        'stu_q_now_acc', 'stu_q_now_time', 'stu_q_now_len', \n",
    "        'q_now_acc', 'q_now_use_time', 'q_now_len', \n",
    "        'kp_now_acc', 'kp_now_use_time','kp_now_len', \n",
    "        'q_all_acc', 'q_all_use_time', 'q_all_len', \n",
    "        'kp_all_acc','kp_all_use_time', 'kp_all_len', \n",
    "        'stu_kp_now_acc_an','stu_kp_now_use_time_an', 'stu_kp_now_len_an', \n",
    "        'stu_q_now_acc_an','stu_q_now_time_an', 'stu_q_now_len_an', \n",
    "        'q_now_acc_an','q_now_use_time_an', 'q_now_len_an', \n",
    "        'kp_now_acc_an','kp_now_use_time_an', 'kp_now_len_an',\n",
    "        'last_kp_time_passed','last_q_time_passed','passed_kp','passed_q'\n",
    "    ]\n",
    "    processed_data = raw_data.copy()\n",
    "    for j, one_col in enumerate(new_col):\n",
    "        processed_data[one_col] = all_data_2[:, j]\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "\n",
    "data_name = 'MOOC'\n",
    "processed_data_name = f'data/kt/{data_name}_info.pkl'\n",
    "print(f'{data_name} processing')\n",
    "raw_data = user_question.copy()\n",
    "raw_data.columns = ['q_id', 'stu_id', 'is_right', 'create_time', 'seq_id','course_id', 'kp_id', 'use_time']\n",
    "print(raw_data.head())\n",
    "stu_kp_data = {}\n",
    "stu_q_data = {}\n",
    "kp_data = {}\n",
    "q_data = {}\n",
    "q_all_data = {}\n",
    "kp_all_data = {}\n",
    "stu_kp_data_an = {}\n",
    "stu_q_data_an = {}\n",
    "kp_data_an = {}\n",
    "q_data_an = {}\n",
    "last_kp_time = {}\n",
    "last_q_time = {}\n",
    "passed_kp_list = {}\n",
    "passed_q_list = {}\n",
    "processed_data= get_data(raw_data, stu_kp_data, stu_q_data, kp_data, q_data, q_all_data,kp_all_data, stu_kp_data_an, stu_q_data_an, kp_data_an, q_data_an,last_kp_time,last_q_time,passed_kp_list,passed_q_list)\n",
    "\n",
    "processed_data = processed_data.reset_index(drop=True)\n",
    "def get_sec(one_d):\n",
    "    if one_d != 0:\n",
    "        one_d = one_d.total_seconds()\n",
    "        return one_d\n",
    "    else:\n",
    "        return 1\n",
    "processed_data['last_kp_time_passed'] = processed_data['last_kp_time_passed'].apply(get_sec)\n",
    "processed_data['last_q_time_passed'] = processed_data['last_q_time_passed'].apply(get_sec)\n",
    "processed_data.to_pickle(processed_data_name)\n",
    "print(data_name, 'Side information extraction processing completed！file at：', processed_data_name)\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:43:34.834834Z",
     "start_time": "2025-03-11T13:43:34.086076Z"
    }
   },
   "outputs": [],
   "source": [
    "USE_ITEM = ['stu_kp_now_acc',\n",
    "            'stu_kp_now_use_time',\n",
    "            'stu_kp_now_len',\n",
    "            'stu_q_now_acc',\n",
    "            'stu_q_now_time',\n",
    "            'stu_q_now_len',\n",
    "            'q_now_acc',\n",
    "            'q_now_use_time',\n",
    "            'q_now_len',\n",
    "            'kp_now_acc',\n",
    "            'kp_now_use_time',\n",
    "            'kp_now_len',\n",
    "            'last_kp_time_passed',\n",
    "            'last_q_time_passed',\n",
    "            'is_right'\n",
    "           ]\n",
    "need_Item = ['stu_id','q_id','kp_id','course_id','seq_id','create_time']\n",
    "need_Item.extend(USE_ITEM)\n",
    "DATASET_NAME = 'MOOC'\n",
    "RAW_DATA = processed_data.copy()[need_Item]\n",
    "RAW_DATA = RAW_DATA.reset_index()\n",
    "raw_data_ts = RAW_DATA['create_time'].copy()\n",
    "RAW_DATA['create_time'] = 0\n",
    "RAW_DATA = RAW_DATA.astype(np.float32)\n",
    "RAW_DATA['create_time'] = raw_data_ts\n",
    "RAW_DATA.index = list(RAW_DATA['stu_id'])\n",
    "ALL_STU_ID_kt = list(RAW_DATA['stu_id'].value_counts().index)\n",
    "X_train, X_test = train_test_split(ALL_STU_ID_kt, random_state=2024, test_size=0.2, shuffle=True)\n",
    "TRAIN_DATA_KT = RAW_DATA.loc[X_train].copy().reset_index(drop=True)\n",
    "VAIL_DATA_KT = RAW_DATA.loc[X_test].copy().reset_index(drop=True)\n",
    "BATCH_SIZE = 64\n",
    "MAX_STEP = 50\n",
    "LR = 0.0001\n",
    "trian_data = MyDataset_kt(TRAIN_DATA_KT)\n",
    "train_data_loader = Data.DataLoader(trian_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "vail_data = MyDataset_kt(VAIL_DATA_KT)\n",
    "vail_data_loader = Data.DataLoader(vail_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:52:42.505756Z",
     "start_time": "2025-04-04T08:52:42.496679Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 设置日志文件\n",
    "MODEL_NAME = 'SIKT'\n",
    "MODEL_INFO = '测试'\n",
    "EPOCH_NUM = 20\n",
    "logging.shutdown()\n",
    "time_now = datetime.datetime.now()\n",
    "now_str = time_now.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "log_dir = f'log/kt'\n",
    "log_file_name = f'log/kt/{MODEL_NAME}_{now_str}_{MODEL_INFO}.log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "file_logger = logging.getLogger(log_file_name)\n",
    "# 向文件输出日志信息\n",
    "file_handler = logging.FileHandler(log_file_name, mode='a', encoding='utf-8')\n",
    "file_handler.setFormatter(formatter)\n",
    "file_logger.addHandler(file_handler)\n",
    "file_logger.info(f\"{config}\")\n",
    "# 模型保存\n",
    "model_dir = f'log/kt/Checkpoints' \n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "best_model_path = f'{model_dir}/{MODEL_NAME}_{now_str}.pth.tar'\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "model = MyModel(input_dim=len(USE_ITEM)-1)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "file_logger.info(f'best_model_path:{best_model_path}')\n",
    "best_auc = 0\n",
    "model.to(DEVICE)\n",
    "for one_epoch in range(EPOCH_NUM):\n",
    "    ground_truth = torch.tensor([])\n",
    "    prediction = torch.tensor([])\n",
    "    model.train()\n",
    "    for batch in tqdm(train_data_loader, desc='Training:    ', mininterval=2):\n",
    "        item, label_batch,one_indexs = batch\n",
    "        item, label_batch = item.to(DEVICE), label_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred_batch = model(item)\n",
    "        loss = loss_func(pred_batch, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prediction = torch.cat([prediction, pred_batch.to('cpu')])\n",
    "        ground_truth = torch.cat([ground_truth, label_batch.to('cpu')])\n",
    "    auc, f1, recall, precision, acc,rmse  = performance_kt(ground_truth, prediction)\n",
    "    file_logger.info(f'Train {one_epoch}: auc:{auc},f1:{f1},recall:{recall},precision:{precision},acc:{acc},rmse:{rmse}')\n",
    "    # 验证\n",
    "    ground_truth = torch.tensor([])\n",
    "    prediction = torch.tensor([])\n",
    "    model.eval()\n",
    "    for batch in tqdm(vail_data_loader, desc='valid:    ', mininterval=2):\n",
    "        with torch.no_grad():\n",
    "            item, label_batch,one_indexs = batch\n",
    "            item = item.to(DEVICE)\n",
    "            pred_batch = model(item)\n",
    "            prediction = torch.cat([prediction, pred_batch.to('cpu')])\n",
    "            ground_truth = torch.cat([ground_truth, label_batch])\n",
    "    auc, f1, recall, precision, acc,rmse  = performance_kt(ground_truth, prediction)\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "#         best_model = model\n",
    "        file_logger.info(f'best auc 【{one_epoch}】')\n",
    "        torch.save({'state_dict': model.state_dict()}, best_model_path)\n",
    "    file_logger.info(f'valid {one_epoch}: auc:{auc},f1:{f1},recall:{recall},precision:{precision},acc:{acc},rmse:{rmse}')\n",
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:47:31.349070Z",
     "start_time": "2025-03-11T13:46:41.092335Z"
    }
   },
   "outputs": [],
   "source": [
    "risk_stu_have_q = list(set(list(RAW_DATA['stu_id']))&set(ALL_risk_stu))\n",
    "checkpoint = __load_model__(best_model_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "ALL_Pred = []\n",
    "STU_KP_LEARN = {}\n",
    "for one_stu in tqdm(risk_stu_have_q):\n",
    "    TEST_DATA,one_stu_learn_kp,one_stu_no_learn_kp = get_risk_stu(one_stu,stu_risk_week_num[one_stu])\n",
    "    STU_KP_LEARN[one_stu] = {'learned_kp':one_stu_learn_kp,'unlearned_kp':one_stu_no_learn_kp}\n",
    "    test_data = MyDataset_kt(TEST_DATA)\n",
    "    test_data_loader = Data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    ground_truth = torch.tensor([])\n",
    "    prediction = torch.tensor([])\n",
    "    all_indexs = torch.tensor([])\n",
    "    for batch in test_data_loader:\n",
    "        with torch.no_grad():\n",
    "            item, label_batch,one_indexs = batch\n",
    "            item = item.to(DEVICE)\n",
    "            pred_batch = model(item)\n",
    "            prediction = torch.cat([prediction, pred_batch.to('cpu')])\n",
    "            ground_truth = torch.cat([ground_truth, label_batch])\n",
    "            all_indexs = torch.cat([all_indexs, one_indexs])\n",
    "    all_indexs = all_indexs.tolist()\n",
    "    prediction = prediction.tolist()\n",
    "    TEST_DATA['pred_y'] = prediction\n",
    "    ALL_Pred.append(TEST_DATA)\n",
    "ALL_Pred = pd.concat(ALL_Pred).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:53:32.877478Z",
     "start_time": "2025-04-04T08:53:32.873338Z"
    }
   },
   "outputs": [],
   "source": [
    "STU_KP_master = ALL_Pred.groupby(['stu_id','kp_id'])['pred_y'].mean().rename('master')\n",
    "STU_KP_master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:53:41.741617Z",
     "start_time": "2025-04-04T08:53:41.736521Z"
    }
   },
   "outputs": [],
   "source": [
    "STU_KP_master_flat = STU_KP_master.reset_index()\n",
    "stu_2_course = dict(zip(studentInfo['stu_id'],studentInfo['course_id']))\n",
    "STU_KP_master_flat['course_id'] = STU_KP_master_flat['stu_id'].map(stu_2_course)\n",
    "STU_KP_master_flat = STU_KP_master_flat.sort_values('master')\n",
    "STU_KP_master_flat =STU_KP_master_flat.reset_index(drop=True)\n",
    "def add_unlearned(one_row):\n",
    "    one_data = STU_KP_LEARN[one_row['stu_id']]\n",
    "    if one_row['kp_id'] in one_data['unlearned_kp']:\n",
    "        return -1\n",
    "    else:\n",
    "        return one_row['master']\n",
    "STU_KP_master_flat['master_un'] = STU_KP_master_flat.apply(add_unlearned,axis=1)\n",
    "STU_KP_master_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习资源推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:47:31.505069Z",
     "start_time": "2025-03-11T13:47:31.498856Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_uppercase_sequence(length):\n",
    "    uppercase_letters = string.ascii_uppercase\n",
    "    sequence = []\n",
    "    for i in range(length):\n",
    "        sequence.append(uppercase_letters[i % 26])\n",
    "    return sequence\n",
    "\n",
    "def get_seq(one_seq):\n",
    "    one_real_seq = []\n",
    "    for one in one_seq:\n",
    "        one_real_seq.append(event_code_A[one])\n",
    "    return one_real_seq\n",
    "\n",
    "def sort_dict(one_dict,reverse=True):\n",
    "    return dict(sorted(one_dict.items(), key=lambda x: x[1],reverse=True))\n",
    "\n",
    "def get_dup_seq(one_seq,k):\n",
    "    one_dup_seq = {}\n",
    "    for i in range(len(one_seq) - k + 1):\n",
    "        subseq = one_seq[i:i+k]\n",
    "        seq_click = 1\n",
    "        if subseq in one_dup_seq:\n",
    "            one_dup_seq[subseq] += seq_click\n",
    "        else:\n",
    "            one_dup_seq[subseq] = seq_click\n",
    "    one_dup_seq = sort_dict(one_dup_seq)\n",
    "    return one_dup_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:47:31.694356Z",
     "start_time": "2025-03-11T13:47:31.506478Z"
    }
   },
   "outputs": [],
   "source": [
    "N_LIST =  [5,15,25,35,45,55,65,75,85,95]\n",
    "M = 40\n",
    "K = 4\n",
    "LBP_LEN = 4\n",
    "USE_TF_IDF = True\n",
    "USE_MODULE_INFO = False\n",
    "USE_CLUSTER = True\n",
    "resource_counts_per_student = studentVle.groupby('stu_id')['id_site'].nunique()\n",
    "average_resources_per_student = resource_counts_per_student.mean()\n",
    "print(f'平均每个学生学习过的资源个数: {average_resources_per_student}')\n",
    "all_act_type = list(studentVle['activity_type'].value_counts().index)\n",
    "event_code = dict(zip(all_act_type,generate_uppercase_sequence(len(all_act_type))))\n",
    "event_code_A = dict(zip(generate_uppercase_sequence(len(all_act_type)),all_act_type))\n",
    "studentVle['event_code'] = studentVle['activity_type'].map(event_code)\n",
    "studentVle.index = list(studentVle['stu_id'])\n",
    "student_module = studentVle[['course_id','stu_id']].drop_duplicates()\n",
    "studentInfo_one_hot = pd.get_dummies(student_module,columns=['course_id'])\n",
    "studentInfo_one_hot = studentInfo_one_hot.groupby('stu_id').sum()\n",
    "studentInfo_one_hot.index = list(studentInfo_one_hot.index)\n",
    "# studentInfo_one_hot.head()\n",
    "studentVle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:47:34.528256Z",
     "start_time": "2025-03-11T13:47:31.700123Z"
    }
   },
   "outputs": [],
   "source": [
    "all_seq_data = {}\n",
    "stu_resource = {}\n",
    "need_stu_have = list(studentVle['stu_id'].value_counts().index)\n",
    "for one_stu in tqdm(need_stu_have):\n",
    "    one_stu_data = studentVle.loc[one_stu].copy()\n",
    "    one_num = dict(one_stu_data['activity_type'].value_counts())\n",
    "    one_stu_play = one_stu_data[one_stu_data['activity_type'].isin(['video','question'])].copy()\n",
    "    \n",
    "    one_stu_res = dict(one_stu_play['id_site'].value_counts())\n",
    "    stu_resource[one_stu] = one_stu_res\n",
    "    all_seq_data[one_stu] = list(one_stu_data['event_code'])\n",
    "\n",
    "all_stu_seq = {}\n",
    "all_seq_num = {}\n",
    "for one_stu,one_seq in tqdm(all_seq_data.items()):\n",
    "    all_stu_seq[one_stu] = {}\n",
    "    one_dup_seq = get_dup_seq(''.join(one_seq),LBP_LEN)\n",
    "    all_stu_seq[one_stu] = one_dup_seq\n",
    "    for one_sub_key in one_dup_seq.keys():\n",
    "        if one_sub_key in all_seq_num:\n",
    "            all_seq_num[one_sub_key] += 1\n",
    "        else:\n",
    "            all_seq_num[one_sub_key] = 1\n",
    "\n",
    "if LBP_LEN >2:\n",
    "    all_seq_num = {k:v for k, v in all_seq_num.items() if v>=100}\n",
    "    \n",
    "\n",
    "all_stu_num = len(all_seq_data.keys())\n",
    "seq_IDF = [math.log(all_stu_num/(one+1),10) for one in list(all_seq_num.values())]\n",
    "all_seq_IDF = dict(zip(list(all_seq_num.keys()),seq_IDF))\n",
    "stu_TF_IDF = {}\n",
    "for key,sub_seq in tqdm(all_stu_seq.items()):\n",
    "    one_stu_TF_IDF = {}\n",
    "    seq_all_num = sum(list(sub_seq.values()))\n",
    "    for sub_k,sub_v in sub_seq.items():\n",
    "        if sub_k not in all_seq_IDF:\n",
    "            continue\n",
    "        one_TF = sub_v/seq_all_num\n",
    "        one_stu_TF_IDF[sub_k] = all_seq_IDF[sub_k]*one_TF\n",
    "    stu_TF_IDF[key] = one_stu_TF_IDF\n",
    "\n",
    "need_feature = dict(zip(list(all_seq_num.keys()),[0]*len(all_seq_num)))\n",
    "all_feature_values = []\n",
    "for one_stu,one_TF_IDF in tqdm(stu_TF_IDF.items()):\n",
    "    one_need_feature = need_feature.copy()\n",
    "    if USE_TF_IDF:\n",
    "        one_need_feature.update(one_TF_IDF)\n",
    "    else:\n",
    "        one_need_feature.update(all_stu_seq[one_stu])\n",
    "    all_feature_values.append(one_need_feature)\n",
    "stu_id_index = list(stu_TF_IDF.keys())\n",
    "my_data = pd.DataFrame(all_feature_values,index=stu_id_index)\n",
    "my_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:47:37.624477Z",
     "start_time": "2025-03-11T13:47:34.529549Z"
    }
   },
   "outputs": [],
   "source": [
    "if USE_MODULE_INFO:\n",
    "    my_data = pd.concat([my_data,studentInfo_one_hot.loc[stu_id_index]],join='inner',axis=1)\n",
    "    my_data.head()\n",
    "    \n",
    "if USE_CLUSTER:\n",
    "    tsne = manifold.TSNE(n_components=2,learning_rate='auto', init='pca', random_state=42).fit_transform(my_data)\n",
    "    KM_cluster = KMeans(init='k-means++', n_clusters=K,random_state=2023)\n",
    "    KM_cluster.fit(tsne)\n",
    "    all_labels = list(KM_cluster.labels_)\n",
    "    stu_2_cluster = dict(zip(stu_id_index,all_labels))\n",
    "\n",
    "start_time = time.time()\n",
    "all_stu_pearson = {}\n",
    "if USE_CLUSTER:\n",
    "    cluster_data = my_data.copy()\n",
    "    cluster_data['cluster'] = all_labels\n",
    "    nee_col = list(cluster_data.columns[:-1])\n",
    "    for one_cluster in tqdm(range(K)):\n",
    "        one_clus_data = cluster_data[cluster_data['cluster']==one_cluster][nee_col]\n",
    "        all_stu_pearson[one_cluster] = one_clus_data.T.corr(method='pearson')\n",
    "    display(all_stu_pearson[0].head())\n",
    "else:\n",
    "    all_stu_pearson =  my_data.T.corr(method='pearson')\n",
    "    display(all_stu_pearson.head())\n",
    "end_time = time.time()\n",
    "run_time = end_time - start_time\n",
    "print('run time：',run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:47:39.145843Z",
     "start_time": "2025-03-11T13:47:37.626322Z"
    }
   },
   "outputs": [],
   "source": [
    "all_stu_sim_resource = {}\n",
    "for one_stu in tqdm(stu_id_index):\n",
    "    if USE_CLUSTER:\n",
    "        one_cluster = stu_2_cluster[one_stu]\n",
    "        k_person = dict(all_stu_pearson[one_cluster].loc[one_stu].sort_values(ascending=False)[:M+1])\n",
    "    else:\n",
    "        k_person = dict(all_stu_pearson.loc[one_stu].sort_values(ascending=False)[:M+1])\n",
    "    # 去掉自己\n",
    "    k_person[one_stu] = 10\n",
    "    del k_person[one_stu]\n",
    "    one_stu_resource = {}\n",
    "    for sim_stu,sim in k_person.items():\n",
    "        one_sim_stu_resource = list(stu_resource[sim_stu].keys())\n",
    "        for one_resource in one_sim_stu_resource:\n",
    "            if one_resource not in one_stu_resource:\n",
    "                one_stu_resource[one_resource] = 0\n",
    "            one_stu_resource[one_resource] += sim\n",
    "    all_stu_sim_resource[one_stu] = sorted(one_stu_resource.items(), key= lambda k:k[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:55:03.478247Z",
     "start_time": "2025-04-04T08:55:03.472020Z"
    }
   },
   "outputs": [],
   "source": [
    "all_re = []\n",
    "for top_n in tqdm(N_LIST):\n",
    "    all_stu_rec_video = {}\n",
    "    for one_stu in stu_id_index:\n",
    "        all_stu_rec_video[one_stu] = all_stu_sim_resource[one_stu][:top_n]\n",
    "    all_recall = []\n",
    "    all_pre = []\n",
    "    all_f1 = []\n",
    "    for one_stu in stu_id_index:\n",
    "        one_train_video = list(stu_resource[one_stu].keys())\n",
    "        one_hit = 0\n",
    "        for one in all_stu_rec_video[one_stu]:\n",
    "            if one[0] in one_train_video:\n",
    "                one_hit += 1\n",
    "        one_recall = one_hit/len(one_train_video)\n",
    "        all_recall.append(one_recall)\n",
    "        if len(all_stu_rec_video[one_stu])==0:\n",
    "            print(one_stu)\n",
    "        one_pre = one_hit/len(all_stu_rec_video[one_stu])\n",
    "        all_pre.append(one_pre)\n",
    "        if (one_recall+one_pre)==0:\n",
    "            one_f1 = 0\n",
    "        else:\n",
    "            one_f1 = (2*one_recall*one_pre)/(one_recall+one_pre)\n",
    "        all_f1.append(one_f1)\n",
    "    all_recall_1 = np.mean(all_recall)\n",
    "    all_pre_1 = np.mean(all_pre)\n",
    "    all_f1_1 = np.mean(all_f1)\n",
    "    one_re = {'Top-N':top_n,'Precision':all_pre_1,'Recall':all_recall_1,'F1':all_f1_1,}\n",
    "    all_re.append(one_re)\n",
    "all_re_set = pd.DataFrame(all_re)\n",
    "all_re_set.index = list(all_re_set['Top-N'])\n",
    "all_re_set.to_csv(f'log/lrr/K.{K}-LBP.{LBP_LEN}-TF.{USE_TF_IDF}-MODULE.{USE_MODULE_INFO}-CLUSTER.{USE_CLUSTER}.csv',index=False)\n",
    "all_re_set.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抽取学习路径规划信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:50:45.423151Z",
     "start_time": "2025-03-11T13:50:45.414821Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_video_list(stu_id):\n",
    "    one_stu_video = all_stu_rec_video[stu_id]\n",
    "    need_video = []\n",
    "    get_video_num = 0\n",
    "    for one_v in one_stu_video:\n",
    "        one_video = dict(need_resource.loc[one_v[0]])\n",
    "        if one_video['re_type'] =='V' and one_video['course_id']==stu_2_course[stu_id]:\n",
    "            get_video_num += 1\n",
    "            need_video.append(one_video)\n",
    "        if get_video_num == VIDEO_LEN:\n",
    "            break\n",
    "    return need_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:50:48.540586Z",
     "start_time": "2025-03-11T13:50:48.526871Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_weak_kp(stu_id,week_num):    \n",
    "    stu_learn_kp_list = STU_KP_LEARN[stu_id]['learned_kp']\n",
    "    stu_unlearn_kp_list = STU_KP_LEARN[stu_id]['unlearned_kp']\n",
    "    \n",
    "    stu_m_kp = STU_KP_master_flat[STU_KP_master_flat['stu_id']==stu_id].copy()\n",
    "    stu_m_kp.index = stu_m_kp['kp_id']\n",
    "    # 找出所有的薄弱知识点\n",
    "    stu_have_weak_kp = list(stu_m_kp[stu_m_kp['master']<KP_WEAK_RATE].index)\n",
    "    # 找出已学且薄弱的知识\n",
    "    need_kp_list = list(set(stu_learn_kp_list)&set(stu_have_weak_kp))\n",
    "    if len(need_kp_list)==0:\n",
    "        need_kp_list = stu_have_weak_kp[:5]\n",
    "    # 薄弱知识点\n",
    "    kp_weak = [{'id':one,\n",
    "                'master':stu_m_kp.loc[one]['master'],\n",
    "                'name':kp_id_2_name[one],\n",
    "                } for one in need_kp_list]\n",
    "    #  已掌握知识点\n",
    "    kp_master_list = list(stu_m_kp[stu_m_kp['master']>=KP_WEAK_RATE].index)\n",
    "    kp_master_list = list(set(stu_learn_kp_list)&set(kp_master_list))\n",
    "    kp_master = [{'id':one,\n",
    "                'master':stu_m_kp.loc[one]['master'],\n",
    "                'name':kp_id_2_name[one],\n",
    "                } for one in kp_master_list]\n",
    "    #  未学习知识点\n",
    "    kp_unlearned = [{'id':one,\n",
    "                'master':stu_m_kp.loc[one]['master'],\n",
    "                'name':kp_id_2_name[one],\n",
    "                } for one in stu_unlearn_kp_list]\n",
    "    return kp_weak,kp_master,kp_unlearned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:50:53.829701Z",
     "start_time": "2025-03-11T13:50:51.406785Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ALL_STU_PATH_INFO = []\n",
    "need_stu_info = studentInfo[['stu_id','course_id','video_t_mean']].copy()\n",
    "re_need_col = ['course_id','re_id','re_type','kp_id','video_t_mean',\n",
    "          'kp_0','kp_1','kp_2','kp_3','kp_0_id','kp_1_id','kp_2_id','kp_3_id']\n",
    "need_resource = resource[re_need_col].copy()\n",
    "need_resource.index = need_resource['re_id'].tolist()\n",
    "KP_WEAK_RATE = 0.999\n",
    "VIDEO_LEN = 10\n",
    "for one_stu_id in tqdm(risk_stu_have_q):\n",
    "    # 首先定位危险周数\n",
    "    one_stu_risk = dict(need_stu_info.loc[one_stu_id]).copy()\n",
    "    week_num = stu_risk_week_num[one_stu_id]\n",
    "    one_stu_risk['risk_week'] = week_num\n",
    "    # 加入课程信息\n",
    "    one_course = course.loc[one_stu_risk['course_id']]\n",
    "    one_stu_risk['course_name'] = one_course['name']\n",
    "    one_stu_risk['course_about'] = one_course['about']\n",
    "    one_stu_risk['kp_weak_list'],one_stu_risk['kp_master_list'],one_stu_risk['kp_unlearned_list'] = get_weak_kp(one_stu_id,week_num)\n",
    "    one_stu_risk['video_list_rec'] = get_video_list(one_stu_id)\n",
    "    ALL_STU_PATH_INFO.append(one_stu_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:50:56.841557Z",
     "start_time": "2025-03-11T13:50:56.799621Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('path_data/real_data.txt','w',encoding='utf-8') as f:\n",
    "    f.write(str(ALL_STU_PATH_INFO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-11T10:14:55.928Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('real_data.txt','r',encoding='utf-8') as f:\n",
    "    real_data = f.read()\n",
    "eval(real_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xhx_kt_py38",
   "language": "python",
   "name": "xhx_kt_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "328.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
